{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Project-02.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhP7Lf1cIZ-",
        "colab_type": "text"
      },
      "source": [
        "## Course Project: Text Classification with Rakuten France Product Data\n",
        "\n",
        "The project focuses on the topic of large-scale product type code text classification where the goal is to predict each product’s type code as defined in the catalog of Rakuten France. This project is derived from a data challenge proposed by Rakuten Institute of Technology, Paris. Details of the data challenge is [available in this link](https://challengedata.ens.fr/challenges/35).\n",
        "\n",
        "The above data challenge focuses on multimodal product type code classification using text and image data. **For this project we will work with only text part of the data.**\n",
        "\n",
        "Please read carefully the description of the challenge provided in the above link. **You can disregard any information related to the image part of the data.**\n",
        "\n",
        "### To obtain the data\n",
        "You have to register yourself [in this link](https://challengedata.ens.fr/challenges/35) to get access to the data.\n",
        "\n",
        "For this project you will only need the text data. Download the training files `x_train` and `y_train`, containing the item texts, and the corresponding product type code labels.\n",
        "\n",
        "### Pandas for handling the data\n",
        "The files you obtained are in CSV format. We strongly suggest to use Python Pandas package to load and visualize the data. [Here is a basic tutorial](https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/) on how to handle data in CSV file using Pandas.\n",
        "\n",
        "If you open the `x_train` dataset using Pandas, you will find that it contains following columns:\n",
        "1. an integer ID for the product\n",
        "2. **designation** - The product title\n",
        "3. description\n",
        "4. productid\n",
        "5. imageid\n",
        "\n",
        "For this project we will only need the integer ID and the designation. You can [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) the other columns.\n",
        "\n",
        "The training output file `y_train.csv` contains the **prdtypecode**, the target/output variable for the classification task, for each integer id in the training input file `X_train.csv`.\n",
        "\n",
        "### Task for the break\n",
        "1. Register yourself and download the training and test for text data. You do not need the `supplementary files` for this project.\n",
        "2. Load the data using pandas and disregard unnecessary columns as mentioned above.\n",
        "3. On the **designation** column, apply the preprocessing techniques.\n",
        "\n",
        "### Task for the end of the course\n",
        "After this preprocessing step, you have now access to a TF-IDF matrix that constitute our data set for the final evaluation project. The project guidelines are:\n",
        "1. Apply all approaches taught in the course and practiced in lab sessions (Decision Trees, Bagging, Random forests, Boosting, Gradient Boosted Trees, AdaBoost, etc.) on this data set. The goal is to predict the target variable (prdtypecode).\n",
        "2. Compare performances of all these models in terms of the weighted-f1 scores you can output. \n",
        "3. Conclude about the most appropriate approach on this data set for the predictive task. \n",
        "4. Write a report in .tex format that adress all these guidelines with a maximal page number of 5 (including figures, tables and references). We will take into account the quality of writing and presentation of the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZUdNYqh3z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "c7bd1ee3-ff15-469c-d32b-1ada9ec3589d"
      },
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.1.0-cp36-none-any.whl size=13156209 sha256=018c97bd35bcafaf4204d12b5729d51947d16ee75da7fdf3f6f8b4415362329d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v91p8919/wheels/ab/82/2a/61dd0ff02e22f10eef65a5aa35453a0eb745c84b4c874b612f\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUfoMyxocIaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import fr_core_news_sm\n",
        "\n",
        "# Load spaCy for french\n",
        "spacy_nlp = fr_core_news_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR_HqRt_euvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e9e4e3e0-2815-4191-d185-de40fc24a757"
      },
      "source": [
        "# linking Google-Drive to save plots\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJdb1QTRfiRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a directory for saving the models and the training progress\n",
        "save_folder = '/content/drive/My Drive/MScDSBA/Data_Science/Ensemble_Learning/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGgaeZqRcIaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download data\n",
        "X_train = pd.read_csv(save_folder + 'data/X_train.csv')\n",
        "Y_train = pd.read_csv(save_folder + 'data/Y_train.csv')\n",
        "X_test = pd.read_csv(save_folder + 'data/X_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Xs6pX0cIaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train.head()\n",
        "# Y_train.head()\n",
        "# X_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlQlsV6xcIaW",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "1. We only keep the designation and id. \n",
        "2. We normalize the accents, put the text in lower-case, remove the punctuation, tokenise the extracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU1JUqUscIaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# designation and ids\n",
        "def cleaning(X_train): \n",
        "    X_train = X_train.drop(['description', 'productid','imageid'], axis=1)\n",
        "    X_train.columns = ['integer_id', 'designation']\n",
        "    return X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rey2YgHKcIab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_accent(string):\n",
        "    string = string.replace('á', 'a')\n",
        "    string = string.replace('â', 'a')\n",
        "\n",
        "    string = string.replace('é', 'e')\n",
        "    string = string.replace('è', 'e')\n",
        "    string = string.replace('ê', 'e')\n",
        "    string = string.replace('ë', 'e')\n",
        "\n",
        "    string = string.replace('î', 'i')\n",
        "    string = string.replace('ï', 'i')\n",
        "\n",
        "    string = string.replace('ö', 'o')\n",
        "    string = string.replace('ô', 'o')\n",
        "    string = string.replace('ò', 'o')\n",
        "    string = string.replace('ó', 'o')\n",
        "\n",
        "    string = string.replace('ù', 'u')\n",
        "    string = string.replace('û', 'u')\n",
        "    string = string.replace('ü', 'u')\n",
        "\n",
        "    string = string.replace('ç', 'c')\n",
        "    \n",
        "    return string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wNlScwQcIae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raw_to_tokens(raw_string, spacy_nlp):\n",
        "    # Write code for lower-casing\n",
        "    string = raw_string.lower()\n",
        "    \n",
        "    # Write code to normalize the accents\n",
        "    string = normalize_accent(string)\n",
        "        \n",
        "    # Write code to tokenize\n",
        "    spacy_tokens = spacy_nlp(string)\n",
        "        \n",
        "    # Write code to remove punctuation tokens and create string tokens\n",
        "    string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct if not token.is_stop]\n",
        "    \n",
        "    # Write code to join the tokens back into a single string\n",
        "    clean_string = \" \".join(string_tokens)\n",
        "    \n",
        "    return clean_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlhkp71NcIak",
        "colab_type": "text"
      },
      "source": [
        "### Apply pre-processing functions\n",
        "\n",
        "N.B.: Section takes about ~20 min to run, skip-it if you have the pre-processed data folders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Ap5EIhcIam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train - step takes roughly ~15:30 min > Uncomment below to run text procesing\n",
        "X_train = cleaning(X_train)\n",
        "X_train['designation_cleaned'] = X_train['designation'].apply(lambda x: raw_to_tokens(x, spacy_nlp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_-Nxa_bcIaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_test - step takes roughly ~2:20 min > Uncomment below to run text procesing\n",
        "X_test = cleaning(X_test)\n",
        "X_test['designation_cleaned'] = X_test['designation'].apply(lambda x: raw_to_tokens(x, spacy_nlp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0CrkgjrcIav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save folders to avoid re-processing everytime\n",
        "X_train.to_csv(r(save_folder + 'data/X_train_cleaned.csv'), index = False, header=True)\n",
        "X_test.to_csv(r(save_folder + 'data/X_test_cleaned.csv'), index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "locvGtwKcIa0",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF matrix\n",
        "\n",
        "Construct the TF-IDF matrix from the pre-processed data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btqxRLUNcIa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.read_csv(save_folder + 'data/X_train_cleaned.csv')\n",
        "X_test = pd.read_csv(save_folder + 'data/X_test_cleaned.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4VbcBOqcIa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list from the processed cells\n",
        "doc_clean =  X_train['designation_cleaned'].astype('U').tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbEVHH7OcIa7",
        "colab_type": "code",
        "outputId": "308c8ef0-3786-45c2-c858-4f21afdf12cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# convert raw documents into TF-IDF matrix.\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(doc_clean)\n",
        "\n",
        "print(\"Shape of the TF-IDF Matrix:\")\n",
        "print(X_tfidf.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the TF-IDF Matrix:\n",
            "(84916, 79402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO0UjVRRcIa_",
        "colab_type": "text"
      },
      "source": [
        "### PCA of the TFIDF matrix \n",
        "We apply a PCA on the TF-IDF matrix to reduce the dimension. Given the matrix is very sparse, this improves the speed of the algorithms. We opt for **XY** principal components corresponding to 85% variance explained. SInce the matrix is very sparse, Sparse PCA model is used. \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sqjhg13cIbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import SparsePCA\n",
        "\n",
        "transformer = SparsePCA(n_components=5, random_state=0)\n",
        "transformer.fit(X_tfidf.toarray())\n",
        "\n",
        "X_transformed = transformer.transform(X_tfidf)\n",
        "X_transformed.shape\n",
        "\n",
        "# most values in the components_ are zero (sparsity)\n",
        "np.mean(transformer.components_ == 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FElzRvq2cIbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_transformed = X_tfidf\n",
        "# print(X_transformed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzjmYOR9cIbJ",
        "colab_type": "text"
      },
      "source": [
        "## Apply various models to predict the target variable\n",
        "1. Decision Trees\n",
        "2. Bagging\n",
        "3. Random forests\n",
        "4. Boosting\n",
        "5. Gradient Boosted Trees\n",
        "6. AdaBoost, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtafdfGAcIbK",
        "colab_type": "text"
      },
      "source": [
        "### 1. Decision trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G15rXjucIbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn.cross_validation import  cross_val_score\n",
        "\n",
        "parameters = param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}\n",
        "grid_dec_tree = GridSearchCV(tree.DecisionTreeClassifier(), parameters, cv = 5, scoring = 'f1')\n",
        "result = grid_dec_tree.fit(X_tfidf, Y_train)\n",
        "\n",
        "# add-in tqdm time taken for the algorithm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkCWgRmVcIbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZU8GpeqcIbQ",
        "colab_type": "text"
      },
      "source": [
        "After this preprocessing step, you have now access to a TF-IDF matrix that constitute our data set for the final evaluation project. The project guidelines are:\n",
        "1. Apply all approaches taught in the course and practiced in lab sessions () on this data set. The goal is to predict the target variable (prdtypecode).\n",
        "2. Compare performances of all these models in terms of the weighted-f1 scores you can output. \n",
        "3. Conclude about the most appropriate approach on this data set for the predictive task. \n",
        "4. Write a report in .tex format that adress all these guidelines with a maximal page number of 5 (including figures, tables and references). We will take into account the quality of writing and presentation of the report."
      ]
    }
  ]
}